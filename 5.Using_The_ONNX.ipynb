{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overview of ONNX Usage\n",
    "\n",
    "#### Cross-Platform Model Deployment\n",
    "* ONNX files can be deployed on a variety of platforms including Windows, Linux, and MacOS, as well as on mobile and embedded systems. They can also be used in cloud environments for scalable inference.\n",
    "\n",
    "#### Framework Interoperability\n",
    "* Models trained in popular frameworks like PyTorch, TensorFlow, and MXNet can be converted into the ONNX format, facilitating use in a different framework that supports ONNX.\n",
    "\n",
    "#### Optimized Inference\n",
    "* The ONNX Runtime (ORT) can be used to execute the models. ORT optimizes model execution automatically, improving performance across hardware platforms.\n",
    "\n",
    "## 2. Converting Models to ONNX Format\n",
    "* To use a model in ONNX format, the first step is to convert the trained model from its native framework. Hereâ€™s how you can convert models from popular frameworks:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.onnx\n",
    "import torch\n",
    "\n",
    "# Assuming 'model' is your PyTorch model and 'example_input' is a tensor appropriate for your model\n",
    "model.eval()\n",
    "torch.onnx.export(model, example_input, \"model.onnx\", export_params=True, opset_version=11)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TensorFlow\n",
    "For TensorFlow models, you will typically use a tool like `tf2onnx`:\n",
    "\n",
    "```bash\n",
    "pip install tf2onnx\n",
    "tf2onnx.convert --saved-model tensorflow-model-directory --output model.onnx\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "# Load the ONNX model\n",
    "session = ort.InferenceSession(\"model.onnx\")\n",
    "\n",
    "# Prepare input data as a dictionary {input_name: input_tensor}\n",
    "inputs = {session.get_inputs()[0].name: input_data}\n",
    "\n",
    "# Run inference\n",
    "outputs = session.run(None, inputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### JavaScript (in Node.js or browser)\n",
    "For running in a JavaScript environment, you can use `onnxjs`:\n",
    "\n",
    "```js\n",
    "\n",
    "import * as onnx from 'onnxjs';\n",
    "\n",
    "async function runModel(inputTensor) {\n",
    "  const session = new onnx.InferenceSession();\n",
    "  await session.loadModel(\"model.onnx\");\n",
    "\n",
    "  const outputMap = await session.run([inputTensor]);\n",
    "  const outputData = outputMap.values().next().value.data;\n",
    "  return outputData;\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Deploying ONNX Models\n",
    "\n",
    "#### Cloud Deployment\n",
    "* You can deploy ONNX models in cloud environments such as AWS, Azure, or Google Cloud, using their respective machine learning services.\n",
    "\n",
    "#### Edge Devices\n",
    "* ONNX models can also be deployed on edge devices like smartphones or IoT devices, typically using optimized versions of ONNX Runtime tailored for these platforms.\n",
    "\n",
    "#### Web Applications\n",
    "* Deploying ONNX models directly in web browsers using JavaScript with libraries like `ONNX.js` allows inference directly in the client's browser.\n",
    "\n",
    "## 5. Advanced Use Cases\n",
    "\n",
    "#### Quantization\n",
    "* You can perform quantization on ONNX models to reduce their size and increase inference speed, especially useful for deployment on resource-constrained devices.\n",
    "\n",
    "#### Model Optimization\n",
    "* Using tools like ONNX Graph Optimizer, you can simplify and optimize the model graph for better performance.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
